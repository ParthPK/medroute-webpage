<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MedRoute, a multi-agent framework for medical diagnosis featuring RL-based Specialist Selection Routing">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MedRoute: RL-Based Dynamic Specialist Routing in Multi-Agent Medical Diagnosis</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MedRoute: RL-Based Dynamic Specialist Routing in Multi-Agent Medical Diagnosis</h1>
          <h2 class="subtitle conference"></h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ashmalvayani.github.io/">Ashmal Vayani*</a>,</span>
            <span class="author-block">
              <a href="https://parthpk.github.io/">Parth Parag Kulkarni*</a>,</span>
            <span class="author-block">
              <a href="https://joefioresi718.github.io/">Joseph Fioresi</a>,</span>
            <span class="author-block">
              <a href="https://songw-sw.github.io/">Song Wang</a>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Institute of Artificial Intelligence, University of Central Florida, USA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1DnFVdQ_ZVWinbZHVqNmHqLYTDuNBbMH_/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1DnFVdQ_ZVWinbZHVqNmHqLYTDuNBbMH_/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://parthpk.github.io/medroute-webpage/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UCF-CRCV/MedRoute/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-video publication-video--portrait" style="padding-bottom: 90%;">
          <img src="static/images/MedAgent_Teaser.jpg" />
        </div>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Medical diagnosis using Large Multimodal Models (LMMs) has gained increasing attention due to capability of these models in 
            providing precise diagnoses. These models generally combine medical questions with visual inputs to generate diagnoses or 
            treatments. However, they are often overly general and unsuitable under the wide range of medical conditions in real-world 
            healthcare. In clinical practice, diagnosis is performed by multiple specialists, each contributing domain-specific expertise. 
            To emulate this process, a potential solution is to deploy a dynamic multi-agent LMM framework, where each agent functions as a 
            medical specialist. Current approaches in this emerging area, typically relying on static or predefined selection of various 
            specialists, cannot be adapted to the changing practical scenario. In this paper, we propose MedRoute, a flexible and dynamic 
            multi-agent framework that comprises of a collaborative system of specialist LMM agents. Furthermore, we add a General 
            Practitioner with an RL-trained router for dynamic specialist selection, and a Moderator that produces the final decision. In 
            this way, our framework closely mirrors real clinical workflows. Extensive evaluations on text and image-based medical datasets 
            demonstrate improved diagnostic accuracy, outperforming the state-of-the-art baselines. Our work lays a strong foundation for 
            future research.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="publication-video" style="padding-bottom: 35%;">
          <img src="static/images/MedAgent_Overall.jpg" />
        </div>
        <div class="content has-text-justified">
          <p>
            The question along with the image is input into the General Practitioner(GP) Agent which is a router for specialist allocation. 
            The GP inputs all potential agents from the specialist pool and allocates the first specialist based on the question 
            (Neurologist in this case). The Neurologist agent gives its diagnosis which is passed into the GP as history, and used by the GP 
            to route to the next specialist(Radiologist here). Radiologist agent's diagnosis acts as history for the GP to consult the third 
            specialist (Neurosurgeon). The process repeats till the GP deems it necessary. Finally all diagnoses from selected specialists 
            are passed on to the Moderator agent which summarizes them and outputs the final decision. The working mechanism of the GP Routing
            process is outlined below.
          </p>  
        </div>
        <div class="publication-video" style="padding-bottom: 28%;">
          <img src="static/images/MedAgent_Router_Only_v2.jpg" />
        </div>
        <div class="content has-text-justified">
          <p>
            The image (if multimodal), is input into the image captioner agent to capture detail for specialist allocation. This caption 
            along with the question is input into the TE(.) to generate a combined task embedding. This is simultaneously used to generate 
            Specialist Vector and Specialist History Vector representing the next specialist and previously selected specialists respectively. 
            At the same time, all 'k' potential candidates from the specialist pool and history (previous diagnoses) are embedded by the 
            SRE(.) and HE(.) respectively. Finally all these 5 embeddings (task, specialist vector, specialist history vector, candidate 
            specialists, history) are concatenated and passed to a Routing Transformer. The output is further passed through an MLP which 
            outputs a k-dimensional vector used to determine the final route.
          </p>  
        </div>
      </div>
    </div>
    <!--/ Method. -->

    <!-- Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Example</h2>
        <div class="publication-video publication-video--portrait" style="padding-bottom: 90%;">
          <img src="static/images/MedAgent_Qual.jpg" />
        </div>
        <div class="content has-text-justified">
          <p>
            The input image shows axial CT scans of the heart and an ECG, which the question asks about. Out of the variety of specialists 
            in the pool, the GP agent first selects a Cardiologist, which analyzes the ECG to give its diagnosis. Based on this diagnosis 
            the GP consults a Thoracic Surgeon that gives a diagnosis consistent with the previous agent. Finally the GP routes to a 
            Hematologist agent which also gives its own diagnosis. All 3 diagnoses are passed on to the Moderator agent that understands and 
            summarizes them to give a final diagnosis “Prominent enlargement of the right atrium and right ventricle”.
          </p>
        </div>
      </div>
    </div>
    <!--/ Dataset. -->
  </div>

    <!-- Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Results</h2>
        <div class="content has-text-justified">
          <p>
            Our model can perform diagnosis on text-based and image-based medical questions. We show our model’s performance compared to 
            other Large Language/Multimodal Models and Multimodal agentic framework on 2 text-only datasets and 3 image-text datasets.
          </p>
          <div class="publication-video" style="padding-bottom: 20.25%;">
            <img src="static/images/MedAgent_result_txtonly.jpg" />
          </div>
          <p>
            Table shows results on text-only datasets (MedQA and PubMedQA). Both datasets are similarly sized at 1273 samples 
            and 1000 samples respectively. Our model outperforms the state-of-the-art model on both datasets by ∼ 6% and ∼ 2% respectively. 
            Out of the baselines GPT-4.1-mini performs the best with 85.86% accurate answers on MedQA, while MAM performs the best on 
            PubMedQA. In general the medical models fare much better than general purpose LLMs (with the exception of GPT) which is the 
            expected trend.
          </p>
          <div class="publication-video" style="padding-bottom: 20.25%;">
            <img src="static/images/MedAgent_result_imgtxt.jpg" />
          </div>
          <p>
            We also evaluate on image-text multimodal medical datasets (PMC-VQA, DeepLesion and PathVQA) as seen in the table. All 
            image-text datasets are larger than their text-only counterparts. Out of the 3, PMC-VQA is the most similar in size with 2000 
            samples, while DeepLesion and PathVQA are much larger with 4927 samples and 6719 samples respectively. Also PMC-VQA consists of 
            general medical questions, DeepLesion has QA pairs constructed from coarse lesion class labels, and PathVQA consists of open-ended 
            questions. We observe that our model again shows substantial improvement on all 3 datasets over state-of-the-art agentic 
            framework, especially in DeepLesion (∼ 5.5%). We observe a mixed trend in vision models where Medical VLMs for some datasets 
            perform better than general purpose VLMs, while performing worse for others.
          </p>
        </div>
      </div>
    </div>
    <!--/ Dataset. -->
  </div>


  
</section>
<!--
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

     
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      

     
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    

    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        

        
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        

      </div>
    </div>
    


    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>
-->  

</body>
</html>
